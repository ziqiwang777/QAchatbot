{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“UNIKEY_COMP5046_Ass1_ipynb”的副本 (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#files - {google drive id : file name}\n",
        "files = {\"1DyV6vxpd5m2VjcWnBs1dR7txf7f3assJ\":\"qna_chitchat_the_comic.tsv\",\\\n",
        "          \"1jo8eb--b7Tj6nJpJ39eYCp4ZiL3Oz6sC\":\"qna_chitchat_the_friend.tsv\",\\\n",
        "          \"12x4QlCZ7nT9CJvOqioN-ZWTwvlKCwlkv\":\"qna_chitchat_the_professional.tsv\"}\n",
        "\n",
        "for fid,fname in files.items():\n",
        "    downloaded = drive.CreateFile({'id':fid}) \n",
        "    downloaded.GetContentFile(fname)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Processing data is divided by several parts.\n",
        "1. get data from 3 files and divided them to 4 lists that one is a list of question sentences (only one is because the questions of 3 files is same) and 3 lists of answer.\n",
        "\n",
        "2. have written a method named \"sentence_process\" to process sentences including removing punctuations, tokenization, lowercase,  stemming and replacement of some special words (for example    's ---> is, 're -->are).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "filesname = list(files.values())\n",
        "# read data by pandas\n",
        "comic = pd.read_csv(filesname[0], sep='\\t')\n",
        "friend = pd.read_csv(filesname[1], sep='\\t')\n",
        "professional = pd.read_csv(filesname[2], sep='\\t')\n",
        "\n",
        "\n",
        "#Questions lists\n",
        "Q = list(comic[\"Question\"])\n",
        "# friend_Q = list(friend[\"Question\"])\n",
        "# professional_Q = list(professional[\"Question\"])\n",
        "\n",
        "#answer lists\n",
        "comic_A = list(comic[\"Answer\"])\n",
        "friend_A = list(friend[\"Answer\"])\n",
        "professional_A = list(professional[\"Answer\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ryxHB4tKd2b",
        "colab_type": "code",
        "outputId": "876813b1-594e-4d6f-9729-6ed483084ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "izWIMnxnPBTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# removing punctuations, tokenization, lowercase, stemming\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)  \n",
        "    return x\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def sentence_process(S):\n",
        "    replace_dic = {\"'s\" : \"is\",\"'re\":\"are\",\"'m\":\"am\",\"n't\":\"not\",\"'ve\":\"have\"}\n",
        "    sentences_list = []\n",
        "    for sentence in S:\n",
        "        sentence_token = []\n",
        "\n",
        "        sentences = word_tokenize(sentence.lower())\n",
        "        for i in range(len(sentences)):\n",
        "            word = sentences[i]\n",
        "            if word in replace_dic.keys():\n",
        "                word = replace_dic[word]\n",
        "            word = remove_punctuation_re(word).strip()\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "    \n",
        "            sentence_token.append(word)\n",
        "        sentence_token = [i for i in sentence_token if i!= '']\n",
        "        sentences_list.append(sentence_token)\n",
        "    return sentences_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPvIRud8Ut_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "This program have implemented Word2Vec with Neural Network with TensorFlow to do word embddings. Because this method transfer the word to vector with strong relationship with the context of this word. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part 2.1.1 is empty because this program use Microsoft BotBuilder Personality Chat Datasets that have downloaded as corpus of Word Embeddings.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QLjf_pm9NiA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Part 2.1.1 is empty because this program use Microsoft BotBuilder Personality Chat Datasets that have downloaded as corpus of Word Embeddings.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.  preprocess the sentence (tokenization,stemming ...) using sentence_process method. First we must do tokenization for sentence to word so that we can get word vector with relationship with nearlist word, which have better effects in training. Stemming is to can make words more concentrative because it can put the diffenernet conversion of same word together. Replacement of abbreviation(such as 's -->is')have same effects.\n",
        "\n",
        "\n",
        "\n",
        "2. generate a dict for {uniqueword :index} -- necessary part\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KRi47n9PKUlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from functools import reduce\n",
        "import random\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_skip_grams(seq):\n",
        "  \n",
        "    '''get_skip_grams is to get the a unique word list and give them a index. \n",
        "    Before giving them a index, I have sorted unique word list to make sure every times\n",
        "    the word in same corpus have same index.\n",
        "    '''\n",
        "  \n",
        "    word_sequence = ' '.join([' '.join(i) for i in sentence_process(seq) ]).split()\n",
        "#     print(word_sequence)\n",
        "    word_list = ' '.join([' '.join(i) for i in sentence_process(seq) ]).split()\n",
        "    word_list = list(set(word_list))\n",
        "\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    skip_grams = []\n",
        "#     print(word_dict)\n",
        "    for i in range(1, len(word_sequence) - 1):\n",
        "        # (context, target) : ([target index - 1, target index + 1], target)\n",
        "        target = word_dict[word_sequence[i]]\n",
        "        context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "        # skipgrams - (target, context[0]), (target, context[1])..\n",
        "        for w in context:\n",
        "            skip_grams.append([target, w])\n",
        "    return word_list,skip_grams\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3iaSY7PAuEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I just test several hyperparameters and choose a set hyperparameters which has less cost and good speed.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_word2vec_attribute(word_list,learning_rate,batch_size,embedding_size,sample_size):\n",
        "    '''\n",
        "    It just uses tf to generate a model without training process.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # sampling size for nce_loss function (cost function)\n",
        "    # must be lower than batch_size\n",
        "    \n",
        "\n",
        "    voc_size = len(word_list)\n",
        "\n",
        "\n",
        "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "\n",
        "    embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0),name = \"embeddings\")\n",
        "\n",
        "    selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "\n",
        "    nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "    nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
        "\n",
        "    cost_op = tf.reduce_mean(\n",
        "                tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, sample_size, voc_size))\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n",
        "    return cost_op,train_op,inputs,labels,embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Training Model\n",
        "\n",
        "def prepare_batch(data, size):\n",
        "  '''\n",
        "  generate random batch used to train model again and again\n",
        "  '''\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        random_inputs.append(data[i][0])  # target\n",
        "        random_labels.append([data[i][1]])  # context word\n",
        "\n",
        "    return random_inputs, random_labels\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIRr3he6cfDK",
        "colab_type": "code",
        "outputId": "66ad6430-3ff3-4d63-e6b8-58b9e365c5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "    give some parameters and train model \n",
        "'''\n",
        "\n",
        "learning_rate = 0.1\n",
        "batch_size = 20\n",
        "embedding_size = 20\n",
        "sample_size = 10\n",
        "word_list,skip_grams = get_skip_grams(Q)\n",
        "cost_op,train_op,inputs,labels,embeddings = generate_word2vec_attribute(word_list,\\\n",
        "                    learning_rate,batch_size,embedding_size,sample_size)\n",
        "init = tf.global_variables_initializer()\n",
        "#     with tf.Session() as sess:\n",
        "try:\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "\n",
        "    no_of_epochs = 800\n",
        "    display_interval = 50\n",
        "\n",
        "    for epoch in range(no_of_epochs):\n",
        "        batch_inputs, batch_labels = prepare_batch(skip_grams, batch_size)\n",
        "        sess.run(train_op, feed_dict={inputs:batch_inputs, labels:batch_labels})    \n",
        "\n",
        "        if epoch % display_interval == 0 :\n",
        "            # calculate the cost/accuracy of the current model\n",
        "            cost = sess.run(cost_op, feed_dict={inputs:batch_inputs,\n",
        "                                                  labels:batch_labels})\n",
        "            print(\"Epoch \" + str(epoch) + \", Cost= \" + \n",
        "                    \"{:.4f}\".format(cost))\n",
        "except:\n",
        "    print(\"error\")\n",
        "trained_embeddings = embeddings.eval(session=sess)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost= 30.4944\n",
            "Epoch 50, Cost= 9.1110\n",
            "Epoch 100, Cost= 8.9145\n",
            "Epoch 150, Cost= 28.8799\n",
            "Epoch 200, Cost= 3.6291\n",
            "Epoch 250, Cost= 2.6112\n",
            "Epoch 300, Cost= 14.7688\n",
            "Epoch 350, Cost= 5.9261\n",
            "Epoch 400, Cost= 3.9246\n",
            "Epoch 450, Cost= 4.1352\n",
            "Epoch 500, Cost= 6.0060\n",
            "Epoch 550, Cost= 2.6765\n",
            "Epoch 600, Cost= 3.9104\n",
            "Epoch 650, Cost= 3.1155\n",
            "Epoch 700, Cost= 3.1735\n",
            "Epoch 750, Cost= 9.0372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Afxp89Xy5065",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jT3uyIMTFunU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "6ook2PwLnisn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, 'embedding/embedding_model')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "U5r9lANPG5ZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b869f1c-9c64-4cdd-db7e-c312e28374f6"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "download embedding_model\n",
        "'''\n",
        "!mkdir ./embedding\n",
        "\n",
        "id = '1IimIqwzcqDO9K1An9ujBFe1MeAdFwa1W'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('./embedding/checkpoint')\n",
        "\n",
        "id = '1Rhcwz0c9mYA6I7i6UnpGNKif5n9yftv9'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('./embedding/embedding_model.data-00000-of-00001')\n",
        "\n",
        "id = '1HacQfLIzT3X6Ja19IyEzPVrpnlYO2R4j'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('./embedding/embedding_model.index')\n",
        "\n",
        "id = '1VE_2oiVGs2_bOKhO5X-7CDDIQdA4yl0z'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('./embedding/embedding_model.meta')\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./embedding’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "outputId": "cde7b2b4-37ec-4d29-a849-9a94bab0cc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "load model\n",
        "\n",
        "'''\n",
        "try:\n",
        "    tf.reset_default_graph()\n",
        "    sess_emb = tf.Session()\n",
        "    saveremb = tf.train.import_meta_graph('/content/embedding/embedding_model.meta')\n",
        "    saveremb.restore(sess_emb, tf.train.latest_checkpoint(\"/content/embedding\"))\n",
        "    trained_embeddings = sess_emb.run('embeddings:0')\n",
        "\n",
        "    print(\"successs\")\n",
        "\n",
        "except:\n",
        "    print(\"error\")  \n",
        "\n",
        "    \n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/embedding/embedding_model\n",
            "successs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part is load the wordembeddding veoctor from the model which will beused in next step.\n",
        "\n",
        "I just test several hyperparameters and choose a set hyperparameters which has less cost and good speed.\n",
        "\n",
        "The details has been written in code annotation.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZL0VVAdtOuUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "45d2e69e-f462-4af1-b264-317f5db0f238"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "generate two random vector for _P_ and _U_\n",
        "'''\n",
        "\n",
        "print(list(np.random.random(embedding_size)))\n",
        "print(list(np.random.random(embedding_size)))\n",
        "\n"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.022726730458917488, 0.8933668314825327, 0.7749990231081528, 0.30155410235102076, 0.7747845085381629, 0.8059262861102974, 0.08557118324298851, 0.24674503750866672, 0.07890170763273863, 0.7831837962663717, 0.08514834968939688, 0.9906793586865673, 0.6743518141321733, 0.03262531970217364, 0.7741880370434864, 0.2305243144551926, 0.9359870435584894, 0.9349607296692247, 0.9491671517698306, 0.5986285094277695]\n",
            "[0.06775553069314133, 0.982955846794942, 0.5443139421747917, 0.7020466816445602, 0.2423378227828037, 0.45742008857563643, 0.3067437132417977, 0.32924339280112735, 0.1745239588589509, 0.15505405601667288, 0.07077158579704246, 0.3655360460320407, 0.6782212648257967, 0.9099380640966421, 0.3999611956962901, 0.7907395335714739, 0.7008774031478886, 0.23181565675756166, 0.6110803698075847, 0.9355485157436025]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        " \n",
        "generate a dict for word vector\n",
        "\n",
        "token_Q_dict = {word:wordvector,...}\n",
        "\n",
        "'''\n",
        "\n",
        "word_list = ' '.join([' '.join(i) for i in sentence_process(Q)]).split()\n",
        "word_list = sorted(set(word_list))\n",
        "embedddingdict = {}\n",
        "for i in range(len(word_list)):\n",
        "    label = word_list[i]\n",
        "    embedddingdict[label] =trained_embeddings[i] \n",
        "embedddingdict['_P_'] = np.array([0.022726730458917488, 0.8933668314825327, 0.7749990231081528, 0.30155410235102076, 0.7747845085381629, 0.8059262861102974, 0.08557118324298851, 0.24674503750866672, 0.07890170763273863, 0.7831837962663717, 0.08514834968939688, 0.9906793586865673, 0.6743518141321733, 0.03262531970217364, 0.7741880370434864, 0.2305243144551926, 0.9359870435584894, 0.9349607296692247, 0.9491671517698306, 0.5986285094277695])\n",
        "embedddingdict['_U_'] = np.array([0.06775553069314133, 0.982955846794942, 0.5443139421747917, 0.7020466816445602, 0.2423378227828037, 0.45742008857563643, 0.3067437132417977, 0.32924339280112735, 0.1745239588589509, 0.15505405601667288, 0.07077158579704246, 0.3655360460320407, 0.6782212648257967, 0.9099380640966421, 0.3999611956962901, 0.7907395335714739, 0.7008774031478886, 0.23181565675756166, 0.6110803698075847, 0.9355485157436025])\n",
        "token_Q_dict=embedddingdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "8dQE3f1oi7s_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. get the maximum of the input word amount, and add padding to the ending of every sentence make them have same size.\n",
        "\n",
        "2. add the begging tag to the answer \n",
        "\n",
        "3. add the ending tag to the target \n",
        "\n",
        "These steps can let model know will to start and end.\n",
        "\n",
        "The detials are written in code annotation."
      ]
    },
    {
      "metadata": {
        "id": "AwYIAKRGq7OQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_vectors_q(sentence):  \n",
        "    '''\n",
        "    process the question, add padding transfer to a vetor of a set of word vectors\n",
        "    \n",
        "    [[word1_vector],[word2_vector],[word3_vector],...[_P_s word vector]]\n",
        "    '''\n",
        "    tokenized_sentence = sentence_process([sentence])[0]\n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    for x in range(diff):\n",
        "        tokenized_sentence.append(\"_P_\" )\n",
        "    data = tokens_to_ids(tokenized_sentence)\n",
        "    return data\n",
        "  \n",
        "\n",
        "# convert tokens to word vector\n",
        "def tokens_to_ids(tokenized_sentence):\n",
        "    '''\n",
        "    convert tokens to word vector\n",
        "\n",
        "    '''\n",
        "    ids = []\n",
        "    for token in tokenized_sentence:\n",
        "        # add word vector of _U_ to list only if the token exists in the dictionary, other wise put special token\n",
        "        if token in token_Q_dict.keys():\n",
        "            ids.append(token_Q_dict[token])\n",
        "        else:\n",
        "            ids.append(token_Q_dict[\"_U_\"])\n",
        "    return np.array(ids)\n",
        "\n",
        "\n",
        "def get_answer_dic(Answer):\n",
        "    A = Answer[:]\n",
        "    A.append(\"_B_\")\n",
        "    A.append(\"_E_\")\n",
        "    A.append(\"_UA_\") # unknown answer : used in input of prediction part\n",
        "\n",
        "    unique_A = sorted(list(set(A)))\n",
        "    answer_dic = {w: i for i, w in enumerate(unique_A)}\n",
        "    return answer_dic\n",
        "\n",
        "# get index vector of answer\n",
        "def get_vectors_a(answer,answer_dic):\n",
        "    # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "\n",
        "    answerlist = [\"_B_\",answer]\n",
        "    vectorsize = len(answer_dic)\n",
        "    \n",
        "    answer_vector = []\n",
        "\n",
        "    for i in range(len(answerlist)):\n",
        "\n",
        "        label = answer_dic[answerlist[i]]\n",
        "\n",
        "        i_vector = np.zeros(vectorsize)\n",
        "        i_vector[label] = 1\n",
        "        answer_vector.append(i_vector)\n",
        "    return np.asarray(answer_vector)\n",
        "\n",
        "def get_target_a(answer,answer_dic):\n",
        "    # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "    \n",
        "    answerlist = [answer,\"_E_\"]\n",
        "    targetlist = [answer_dic[i] for i in answerlist]\n",
        "    return targetlist\n",
        "  \n",
        "# -----------test------------- \n",
        "# for a in comic_A:\n",
        "#     print(get_target(a,comic_a_dic))\n",
        "# for a in Q:\n",
        "#     print(get_vectors_q(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The details has been written in code annotation"
      ]
    },
    {
      "metadata": {
        "id": "OF_yn_8JYvqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(Q,Answer,answer_dic):\n",
        "\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "    for q in Q:\n",
        "        input_data = get_vectors_q(q)\n",
        "        input_batch.append(input_data)\n",
        "\n",
        "    for a in Answer:        \n",
        "        output_data = get_vectors_a(a,answer_dic)        \n",
        "        target = get_target_a(a,answer_dic)\n",
        "        \n",
        "        output_batch.append(output_data)\n",
        "        target_batch.append(target)\n",
        "    m,n = np.shape(input_batch[0])\n",
        "    input_batch_arr = np.zeros((len(input_batch),m,n))\n",
        "    for x in range(len(input_batch)):\n",
        "        for y in range(m):\n",
        "            for z in range(n):\n",
        "                input_batch_arr[x][y][z] = input_batch[x][y][z]\n",
        "    return input_batch_arr, output_batch, target_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjRM0bG85yAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get all Answer dicts \n",
        "comic_a_dic = get_answer_dic(comic_A)\n",
        "friend_a_dic = get_answer_dic(friend_A)\n",
        "professional_a_dic = get_answer_dic(professional_A)\n",
        "\n",
        "# the maximum of input words amount\n",
        "question_len = [len(q.split()) for q in Q]\n",
        "max_input_words_amount = max(question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def buildmodel(answer_dic,name):\n",
        "    '''\n",
        "    build a seq2seq model used by tf\n",
        "    '''\n",
        "    ### Setting Hyperparameters\n",
        "    learning_rate = 0.002\n",
        "    n_hidden = 128\n",
        "    n_class = len(answer_dic)\n",
        "    n_input = 20\n",
        "\n",
        "    ### Neural Network Model\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # encoder/decoder shape = [batch size, time steps, input size]\n",
        "    enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "    dec_input = tf.placeholder(tf.float32, [None, None, n_class])\n",
        "\n",
        "    # target shape = [batch size, time steps]\n",
        "    targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "    # Encoder Cell\n",
        "    with tf.variable_scope('encode'):\n",
        "        enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "        enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "        outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                                dtype=tf.float32)\n",
        "    # Decoder Cell\n",
        "    with tf.variable_scope('decode'):\n",
        "        dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "        dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "        # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "        outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                                initial_state=enc_states,\n",
        "                                                dtype=tf.float32)\n",
        "\n",
        "    model = tf.layers.dense(outputs, n_class, activation=None,name = name)\n",
        "\n",
        "    cost = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=model, labels=targets))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    return optimizer,cost,model,enc_input,dec_input,targets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "016zoCFrTZpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(Q,A,answer_dic,name): \n",
        "    '''\n",
        "    train the model\n",
        "    '''\n",
        "    optimizer,cost,model,enc_input,dec_input,targets = buildmodel(answer_dic,name)\n",
        "    input_batch, output_batch, target_batch = make_batch(Q,A,answer_dic)\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    total_epoch = 5000\n",
        "    for epoch in range(total_epoch):\n",
        "        _, loss = sess.run([optimizer, cost],\n",
        "                           feed_dict={enc_input: input_batch,\n",
        "                                      dec_input: output_batch,\n",
        "                                      targets: target_batch})\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1),\n",
        "                  'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'cost =', '{:.6f}'.format(loss))\n",
        "    print('Training completed')\n",
        "\n",
        "    return sess,model,enc_input,dec_input,targets\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "outputId": "9d9a23d7-59b1-41f6-ed79-ee7ac3810e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "cell_type": "code",
      "source": [
        "# train the comic model\n",
        "sess_c,model_c,enc_input_c,dec_input_c,targets_c = train_model(Q,comic_A,comic_a_dic,name = 'comic')\n",
        "\n",
        "#to get the path of them\n",
        "print(model_c) \n",
        "print(enc_input_c)\n",
        "print(dec_input_c)\n",
        "print(targets_c)\n",
        "\n",
        "#save comic model\n",
        "saver_c = tf.train.Saver()\n",
        "saver_c.save(sess_c, 'comic/comic_seq2seq2')\n",
        "sess_c.close()\n"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 4.719897\n",
            "Epoch: 0101 cost = 1.526133\n",
            "Epoch: 0201 cost = 0.216691\n",
            "Epoch: 0301 cost = 0.043375\n",
            "Epoch: 0401 cost = 0.018985\n",
            "Epoch: 0501 cost = 0.011264\n",
            "Epoch: 0601 cost = 0.009863\n",
            "Epoch: 0701 cost = 0.008553\n",
            "Epoch: 0801 cost = 0.005897\n",
            "Epoch: 0901 cost = 0.006974\n",
            "Epoch: 1001 cost = 0.384791\n",
            "Epoch: 1101 cost = 0.013328\n",
            "Epoch: 1201 cost = 0.008216\n",
            "Epoch: 1301 cost = 0.006396\n",
            "Epoch: 1401 cost = 0.005001\n",
            "Epoch: 1501 cost = 0.005060\n",
            "Epoch: 1601 cost = 0.005089\n",
            "Epoch: 1701 cost = 0.003792\n",
            "Epoch: 1801 cost = 0.004223\n",
            "Epoch: 1901 cost = 0.002723\n",
            "Epoch: 2001 cost = 0.003330\n",
            "Epoch: 2101 cost = 0.003121\n",
            "Epoch: 2201 cost = 0.003847\n",
            "Epoch: 2301 cost = 0.003537\n",
            "Epoch: 2401 cost = 0.003589\n",
            "Epoch: 2501 cost = 0.002772\n",
            "Epoch: 2601 cost = 0.004426\n",
            "Epoch: 2701 cost = 0.003376\n",
            "Epoch: 2801 cost = 0.003856\n",
            "Epoch: 2901 cost = 0.003089\n",
            "Epoch: 3001 cost = 0.003412\n",
            "Epoch: 3101 cost = 0.003131\n",
            "Epoch: 3201 cost = 0.004071\n",
            "Epoch: 3301 cost = 0.001943\n",
            "Epoch: 3401 cost = 0.002293\n",
            "Epoch: 3501 cost = 0.002750\n",
            "Epoch: 3601 cost = 0.002956\n",
            "Epoch: 3701 cost = 0.001595\n",
            "Epoch: 3801 cost = 0.003375\n",
            "Epoch: 3901 cost = 0.002607\n",
            "Epoch: 4001 cost = 0.001660\n",
            "Epoch: 4101 cost = 0.002726\n",
            "Epoch: 4201 cost = 0.003815\n",
            "Epoch: 4301 cost = 0.002244\n",
            "Epoch: 4401 cost = 0.009261\n",
            "Epoch: 4501 cost = 0.004861\n",
            "Epoch: 4601 cost = 0.004256\n",
            "Epoch: 4701 cost = 0.003698\n",
            "Epoch: 4801 cost = 0.004567\n",
            "Epoch: 4901 cost = 0.003602\n",
            "Epoch: 5000 cost = 0.003489\n",
            "Training completed\n",
            "Tensor(\"comic/BiasAdd:0\", shape=(?, ?, 100), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(?, ?, 20), dtype=float32)\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, ?, 100), dtype=float32)\n",
            "Tensor(\"Placeholder_2:0\", shape=(?, ?), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WnaQLCWSY7yJ",
        "colab_type": "code",
        "outputId": "967d83d6-0053-41a8-ea01-6478b94f9d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "cell_type": "code",
      "source": [
        "# train and save the friend model\n",
        "\n",
        "sess_f,model_f,enc_input_f,dec_input_f,targets_f = train_model(Q,friend_A,friend_a_dic,name = 'friend')\n",
        "print(model_f)\n",
        "print(enc_input_f)\n",
        "print(dec_input_f)\n",
        "print(targets_f)\n",
        "saver_f = tf.train.Saver()\n",
        "saver_f.save(sess_f, 'friend/friend_seq2seq2')\n",
        "sess_f.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 4.747009\n",
            "Epoch: 0101 cost = 1.535424\n",
            "Epoch: 0201 cost = 0.338736\n",
            "Epoch: 0301 cost = 0.079752\n",
            "Epoch: 0401 cost = 0.033168\n",
            "Epoch: 0501 cost = 0.018807\n",
            "Epoch: 0601 cost = 0.014001\n",
            "Epoch: 0701 cost = 0.008683\n",
            "Epoch: 0801 cost = 0.007790\n",
            "Epoch: 0901 cost = 0.008761\n",
            "Epoch: 1001 cost = 0.006307\n",
            "Epoch: 1101 cost = 0.005573\n",
            "Epoch: 1201 cost = 0.262809\n",
            "Epoch: 1301 cost = 0.050847\n",
            "Epoch: 1401 cost = 0.016151\n",
            "Epoch: 1501 cost = 0.010754\n",
            "Epoch: 1601 cost = 0.007964\n",
            "Epoch: 1701 cost = 0.007055\n",
            "Epoch: 1801 cost = 0.005928\n",
            "Epoch: 1901 cost = 0.004040\n",
            "Epoch: 2001 cost = 0.004644\n",
            "Epoch: 2101 cost = 0.005936\n",
            "Epoch: 2201 cost = 0.007257\n",
            "Epoch: 2301 cost = 0.004623\n",
            "Epoch: 2401 cost = 0.003080\n",
            "Epoch: 2501 cost = 0.004635\n",
            "Epoch: 2601 cost = 0.003723\n",
            "Epoch: 2701 cost = 0.002264\n",
            "Epoch: 2801 cost = 0.003349\n",
            "Epoch: 2901 cost = 0.003001\n",
            "Epoch: 3001 cost = 0.003290\n",
            "Epoch: 3101 cost = 0.003161\n",
            "Epoch: 3201 cost = 0.005060\n",
            "Epoch: 3301 cost = 0.003840\n",
            "Epoch: 3401 cost = 0.005006\n",
            "Epoch: 3501 cost = 0.003244\n",
            "Epoch: 3601 cost = 0.002393\n",
            "Epoch: 3701 cost = 0.003801\n",
            "Epoch: 3801 cost = 0.003020\n",
            "Epoch: 3901 cost = 0.003538\n",
            "Epoch: 4001 cost = 0.003768\n",
            "Epoch: 4101 cost = 0.003314\n",
            "Epoch: 4201 cost = 0.056796\n",
            "Epoch: 4301 cost = 0.005583\n",
            "Epoch: 4401 cost = 0.005714\n",
            "Epoch: 4501 cost = 0.005710\n",
            "Epoch: 4601 cost = 0.002861\n",
            "Epoch: 4701 cost = 0.002605\n",
            "Epoch: 4801 cost = 0.004363\n",
            "Epoch: 4901 cost = 0.004608\n",
            "Epoch: 5000 cost = 0.004562\n",
            "Training completed\n",
            "Tensor(\"friend/BiasAdd:0\", shape=(?, ?, 101), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(?, ?, 20), dtype=float32)\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, ?, 101), dtype=float32)\n",
            "Tensor(\"Placeholder_2:0\", shape=(?, ?), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DE2GkKw_Y9v3",
        "colab_type": "code",
        "outputId": "84500e9b-6412-4464-fcb0-f5ad0e18eb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "cell_type": "code",
      "source": [
        "# train and save the professional model\n",
        "\n",
        "sess_p,model_p,enc_input_p,dec_input_p,targets_p = train_model(Q,professional_A,professional_a_dic,name = 'professional')\n",
        "print(model_p)\n",
        "print(enc_input_p)\n",
        "print(dec_input_p)\n",
        "print(targets_p)\n",
        "saver_p = tf.train.Saver()\n",
        "saver_p.save(sess_p, 'professional/professional_seq2seq2')\n",
        "sess_p.close()"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 4.665354\n",
            "Epoch: 0101 cost = 1.534642\n",
            "Epoch: 0201 cost = 0.303166\n",
            "Epoch: 0301 cost = 0.054190\n",
            "Epoch: 0401 cost = 0.021928\n",
            "Epoch: 0501 cost = 0.014202\n",
            "Epoch: 0601 cost = 0.011906\n",
            "Epoch: 0701 cost = 0.008891\n",
            "Epoch: 0801 cost = 0.006225\n",
            "Epoch: 0901 cost = 0.005921\n",
            "Epoch: 1001 cost = 0.005090\n",
            "Epoch: 1101 cost = 0.005938\n",
            "Epoch: 1201 cost = 0.005157\n",
            "Epoch: 1301 cost = 0.003748\n",
            "Epoch: 1401 cost = 0.003547\n",
            "Epoch: 1501 cost = 0.004528\n",
            "Epoch: 1601 cost = 0.004957\n",
            "Epoch: 1701 cost = 0.003599\n",
            "Epoch: 1801 cost = 0.003981\n",
            "Epoch: 1901 cost = 2.884200\n",
            "Epoch: 2001 cost = 0.099911\n",
            "Epoch: 2101 cost = 0.030284\n",
            "Epoch: 2201 cost = 0.016777\n",
            "Epoch: 2301 cost = 0.011371\n",
            "Epoch: 2401 cost = 0.009058\n",
            "Epoch: 2501 cost = 0.008006\n",
            "Epoch: 2601 cost = 0.006024\n",
            "Epoch: 2701 cost = 0.007030\n",
            "Epoch: 2801 cost = 0.005173\n",
            "Epoch: 2901 cost = 0.005820\n",
            "Epoch: 3001 cost = 0.005947\n",
            "Epoch: 3101 cost = 0.003822\n",
            "Epoch: 3201 cost = 0.004375\n",
            "Epoch: 3301 cost = 0.004801\n",
            "Epoch: 3401 cost = 0.005106\n",
            "Epoch: 3501 cost = 0.003490\n",
            "Epoch: 3601 cost = 0.003042\n",
            "Epoch: 3701 cost = 0.004541\n",
            "Epoch: 3801 cost = 0.002905\n",
            "Epoch: 3901 cost = 0.003950\n",
            "Epoch: 4001 cost = 0.003327\n",
            "Epoch: 4101 cost = 0.004004\n",
            "Epoch: 4201 cost = 0.002235\n",
            "Epoch: 4301 cost = 0.002782\n",
            "Epoch: 4401 cost = 0.003001\n",
            "Epoch: 4501 cost = 0.004170\n",
            "Epoch: 4601 cost = 0.002896\n",
            "Epoch: 4701 cost = 0.003376\n",
            "Epoch: 4801 cost = 0.003664\n",
            "Epoch: 4901 cost = 0.004578\n",
            "Epoch: 5000 cost = 0.003433\n",
            "Training completed\n",
            "Tensor(\"professional/BiasAdd:0\", shape=(?, ?, 100), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(?, ?, 20), dtype=float32)\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, ?, 100), dtype=float32)\n",
            "Tensor(\"Placeholder_2:0\", shape=(?, ?), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwvSAHO0nRgH",
        "colab_type": "code",
        "outputId": "18e25626-cc75-4c98-9d0c-f9701a6d4060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# this method is to load seq2seq model by modelname\n",
        "\n",
        "def changePersonality(modelname):\n",
        "    pathdict = {'friend':['/content/friend/friend_seq2seq2.meta',\"/content/friend\",'friend/BiasAdd:0'],\\\n",
        "               'comic':['/content/comic/comic_seq2seq2.meta',\"/content/comic\",'comic/BiasAdd:0'],\\\n",
        "               'professional':['/content/professional/professional_seq2seq2.meta',\"/content/professional\",'professional/BiasAdd:0']}\n",
        "    pathlist = pathdict[modelname]\n",
        "    try:\n",
        "        tf.reset_default_graph()\n",
        "        sess_load = tf.Session()\n",
        "        savercom = tf.train.import_meta_graph(pathlist[0])\n",
        "        savercom.restore(sess_load, tf.train.latest_checkpoint(pathlist[1]))\n",
        "        model_load = sess_load.graph.get_tensor_by_name(pathlist[2])\n",
        "        enc_inputload = sess_load.graph.get_tensor_by_name('Placeholder:0')\n",
        "        dec_input_load = sess_load.graph.get_tensor_by_name('Placeholder_1:0')\n",
        "        targets_load = sess_load.graph.get_tensor_by_name('Placeholder_2:0')\n",
        "        print(\"Loading successs!\")\n",
        "\n",
        "        return sess_load,model_load,enc_inputload,dec_input_load,targets_load\n",
        "    except:\n",
        "        print(\"error\")\n",
        "\n",
        "#-------------------- test------------------ \n",
        "sess_load,model_load,enc_inputload,dec_input_load,targets_load = changePersonality('friend')\n",
        "# sess_load,model_load,enc_inputload,dec_input_load,targets_load =changePersonality('comic')\n",
        "# sess_load,model_load,enc_inputload,dec_input_load,targets_load =changePersonality('professional')"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/friend/friend_seq2seq2\n",
            "Loading successs!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aBXA_nmnVjUl",
        "colab_type": "code",
        "outputId": "fbcec431-bc51-4961-b8ef-d2a3369858c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "#--------------------test-----------------------\n",
        "# This block is just for testing the code.\n",
        "\n",
        "def answer(sentence,answer_dic,sessl,model,enc_input,dec_input,targets):\n",
        "    q = [sentence]\n",
        "    a = ['_UA_']\n",
        "    input_batch, output_batch, target_batch = make_batch(q,a,answer_dic)\n",
        "\n",
        "\n",
        "    prediction = tf.argmax(model, 2)\n",
        "    result = sessl.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch,\n",
        "                                 dec_input: output_batch,\n",
        "                                 targets: target_batch})\n",
        "\n",
        "    answerlist = list(answer_dic.keys())\n",
        "    decoded = [answerlist[i] for i in result[0]]\n",
        "    \n",
        "    if \"_E_\" in decoded:\n",
        "        end = decoded.index('_E_')\n",
        "        translated = ' '.join(decoded[:end])\n",
        "    else :\n",
        "        translated = ' '.join(decoded[:])\n",
        "    return translated\n",
        "  \n",
        "  \n",
        "questions = Q[:30]   \n",
        "for q in questions:\n",
        "#     print(q , ' ->', answer(q,comic_a_dic,sess_load,model_load,enc_inputload,dec_input_load,targets_load))\n",
        "    print(q , ' ->', answer(q,friend_a_dic,sess_load,model_load,enc_inputload,dec_input_load,targets_load))\n",
        "#     print(q , ' ->', answer(q,professional_a_dic,sesspro,modelpro,enc_inputpro,dec_inputpro,targetspro))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What's your age?  -> I don't really have an age.\n",
            "Are you young?  -> I don't really have an age.\n",
            "When were you born?  -> I don't really have an age.\n",
            "What age are you?  -> I don't really have an age.\n",
            "Are you old?  -> I don't really have an age.\n",
            "How old are you?  -> I don't really have an age.\n",
            "How long ago were you born?  -> I don't really have an age.\n",
            "Ask me anything  -> I'm a much better answerer than asker.\n",
            "Ask me a question  -> I'm a much better answerer than asker.\n",
            "Can you ask me a question?  -> I'm a much better answerer than asker.\n",
            "Ask me something  -> I'm a much better answerer than asker.\n",
            "What do you want to know about me?  -> I'm a much better answerer than asker.\n",
            "Can you sleep?  -> I don't have the hardware for that.\n",
            "Do you have boogers?  -> I don't have the hardware for that.\n",
            "Don't you ever sleep?  -> I don't have the hardware for that.\n",
            "Do you dream?  -> I don't have the hardware for that.\n",
            "Do you smell?  -> I don't have the hardware for that.\n",
            "Do you sweat?  -> I don't have the hardware for that.\n",
            "Do you get tired?  -> I don't have the hardware for that.\n",
            "Can you sneeze?  -> I don't have the hardware for that.\n",
            "Getting tired of you  -> Swing and a miss.\n",
            "You bore me  -> Swing and a miss.\n",
            "I'm tired of you  -> Swing and a miss.\n",
            "You're so basic  -> Swing and a miss.\n",
            "Basic af  -> Swing and a miss.\n",
            "You're no fun  -> Swing and a miss.\n",
            "Be more fun  -> Swing and a miss.\n",
            "Why are you so boring  -> Swing and a miss.\n",
            "You're so boring  -> Swing and a miss.\n",
            "You're boring  -> Swing and a miss.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "dWB3P8MPjRUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import tensorflow as tf\n",
        "from functools import reduce\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#download the seq2seq by id\n",
        "# !rm -rf ./comic\n",
        "# !rm -rf ./friend\n",
        "# !rm -rf ./professional\n",
        "\n",
        "!mkdir ./comic\n",
        "!mkdir ./friend\n",
        "!mkdir ./professional\n",
        "\n",
        "id1 = '1AvpPPbSEYFbjPQnhX1MYH1Ylu_0sOnMM'\n",
        "downloaded1 = drive.CreateFile({'id':id1}) \n",
        "downloaded1.GetContentFile('./comic/checkpoint')\n",
        "\n",
        "id2 = '1Kiew6DhY1A1lQLD6nsozF9RyTFYQmfeH'\n",
        "downloaded2 = drive.CreateFile({'id':id2}) \n",
        "downloaded2.GetContentFile('./comic/comic_seq2seq2.data-00000-of-00001')\n",
        "\n",
        "id3 = '1Snye7tgHWoSHnCM52JI2h-et21JMfEyG'\n",
        "downloaded3 = drive.CreateFile({'id':id3}) \n",
        "downloaded3.GetContentFile('./comic/comic_seq2seq2.index')\n",
        "\n",
        "id4 = '1qQ96pCtZNf32LVSBD19ZbSIr9dlrO4g8'\n",
        "downloaded4 = drive.CreateFile({'id':id4}) \n",
        "downloaded4.GetContentFile('./comic/comic_seq2seq2.meta')\n",
        "\n",
        "\n",
        "# firend\n",
        "\n",
        "id5 = '1XFle1hOuxUL4suKKJilZTaKrtq6jfiRG'\n",
        "downloaded5 = drive.CreateFile({'id':id5}) \n",
        "downloaded5.GetContentFile('./friend/checkpoint')\n",
        "\n",
        "id6 = '1P4qRbbj5bHANxL2uTimyGy45hyWqLABh'\n",
        "downloaded6 = drive.CreateFile({'id':id6}) \n",
        "downloaded6.GetContentFile('./friend/friend_seq2seq2.data-00000-of-00001')\n",
        "\n",
        "id7 = '1S1u6dxteu1Vu69rdC8yl1GIOVGSwwDkB'\n",
        "downloaded7 = drive.CreateFile({'id':id7}) \n",
        "downloaded7.GetContentFile('./friend/friend_seq2seq2.index')\n",
        "\n",
        "id8 = '1ycYfWeBemoJUN9gR69GVMlRJ-0RrA4t6'\n",
        "downloaded8 = drive.CreateFile({'id':id8}) \n",
        "downloaded8.GetContentFile('./friend/friend_seq2seq2.meta')\n",
        "\n",
        "#professional\n",
        "\n",
        "id9 = '1k72xZ5-1teIYKhaBSdJ-5gTaBWutvY03'\n",
        "downloaded9 = drive.CreateFile({'id':id9}) \n",
        "downloaded9.GetContentFile('./professional/checkpoint')\n",
        "\n",
        "id10 = '1sKHiLbJBp4JOJSxy9DJCYb_8IKw_IF-4'\n",
        "downloaded10 = drive.CreateFile({'id':id10}) \n",
        "downloaded10.GetContentFile('./professional/professional_seq2seq2.data-00000-of-00001')\n",
        "\n",
        "id11 = '1C3fmTbWend3Hl2TARSU-xBqZ1pV0_3qW'\n",
        "downloaded11 = drive.CreateFile({'id':id11}) \n",
        "downloaded11.GetContentFile('./professional/professional_seq2seq2.index')\n",
        "\n",
        "id12 = '1se9aRo_TlW6JS7V4dnYZlAoN_fc0JNK3'\n",
        "downloaded12 = drive.CreateFile({'id':id12}) \n",
        "downloaded12.GetContentFile('./professional/professional_seq2seq2.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "metadata": {
        "id": "v_znAs4gJxCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Description: This block is copying the methods has wrritened in part 2. Because in Chatbot ,there are several methods still will be used."
      ]
    },
    {
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Description: This block is copying the methods has wrritened in part 2. \n",
        "Because in Chatbot ,there are several methods still will be used.\n",
        "'''\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)  \n",
        "    return x\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def sentence_process(S):\n",
        "    replace_dic = {\"'s\" : \"is\",\"'re\":\"are\",\"'m\":\"am\",\"n't\":\"not\",\"'ve\":\"have\"}\n",
        "    sentences_list = []\n",
        "    for sentence in S:\n",
        "        sentence_token = []\n",
        "\n",
        "        sentences = word_tokenize(sentence.lower())\n",
        "        for i in range(len(sentences)):\n",
        "            word = sentences[i]\n",
        "            if word in replace_dic.keys():\n",
        "                word = replace_dic[word]\n",
        "            word = remove_punctuation_re(word).strip()\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "    \n",
        "            sentence_token.append(word)\n",
        "        sentence_token = [i for i in sentence_token if i!= '']\n",
        "        sentences_list.append(sentence_token)\n",
        "    return sentences_list\n",
        "\n",
        "def get_skip_grams(seq):\n",
        "    \n",
        "    word_sequence = ' '.join([' '.join(i) for i in sentence_process(seq) ]).split()\n",
        "    word_list = ' '.join([' '.join(i) for i in sentence_process(seq) ]).split()\n",
        "    word_list = list(set(word_list))\n",
        "\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    skip_grams = []\n",
        "    for i in range(1, len(word_sequence) - 1):\n",
        "        target = word_dict[word_sequence[i]]\n",
        "        context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "        for w in context:\n",
        "            skip_grams.append([target, w])\n",
        "    return word_list,skip_grams\n",
        "\n",
        "def get_vectors_q(sentence):  \n",
        "    tokenized_sentence = sentence_process([sentence])[0]\n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    for x in range(diff):\n",
        "        tokenized_sentence.append(\"_P_\" )\n",
        "    data = tokens_to_ids(tokenized_sentence)\n",
        "    return data\n",
        "def tokens_to_ids(tokenized_sentence):\n",
        "    ids = []\n",
        "    for token in tokenized_sentence:\n",
        "        if token in token_Q_dict.keys():\n",
        "            ids.append(token_Q_dict[token])\n",
        "        else:\n",
        "            ids.append(token_Q_dict[\"_U_\"])\n",
        "    return np.array(ids)\n",
        "\n",
        "\n",
        "def get_answer_dic(Answer):\n",
        "    A = Answer[:]\n",
        "    A.append(\"_B_\")\n",
        "    A.append(\"_E_\")\n",
        "    A.append(\"_UA_\")\n",
        "\n",
        "    unique_A = sorted(list(set(A)))\n",
        "    answer_dic = {w: i for i, w in enumerate(unique_A)}\n",
        "    return answer_dic\n",
        "\n",
        "def get_vectors_a(answer,answer_dic):\n",
        "    answerlist = [\"_B_\",answer]\n",
        "    vectorsize = len(answer_dic)\n",
        "    \n",
        "    answer_vector = []\n",
        "\n",
        "    for i in range(len(answerlist)):\n",
        "\n",
        "        label = answer_dic[answerlist[i]]\n",
        "\n",
        "        i_vector = np.zeros(vectorsize)\n",
        "        i_vector[label] = 1\n",
        "        answer_vector.append(i_vector)\n",
        "    return np.asarray(answer_vector)\n",
        "\n",
        "def get_target_a(answer,answer_dic):    \n",
        "    answerlist = [answer,\"_E_\"]\n",
        "    targetlist = [answer_dic[i] for i in answerlist]\n",
        "    return targetlist\n",
        "def make_batch(Q,Answer,answer_dic):\n",
        "\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "    for q in Q:\n",
        "        input_data = get_vectors_q(q)\n",
        "        input_batch.append(input_data)\n",
        "\n",
        "    for a in Answer:        \n",
        "        output_data = get_vectors_a(a,answer_dic)        \n",
        "        target = get_target_a(a,answer_dic)\n",
        "        \n",
        "        output_batch.append(output_data)\n",
        "        target_batch.append(target)\n",
        "    m,n = np.shape(input_batch[0])\n",
        "    input_batch_arr = np.zeros((len(input_batch),m,n))\n",
        "    for x in range(len(input_batch)):\n",
        "        for y in range(m):\n",
        "            for z in range(n):\n",
        "                input_batch_arr[x][y][z] = input_batch[x][y][z]\n",
        "    return input_batch_arr, output_batch, target_batch\n",
        "\n",
        "def answer(sentence,answer_dic,sessl,model,enc_input,dec_input,targets):\n",
        "\n",
        "    q = [sentence]\n",
        "    a = ['_UA_']\n",
        "    input_batch, output_batch, target_batch = make_batch(q,a,answer_dic)\n",
        "\n",
        "\n",
        "    prediction = tf.argmax(model, 2)\n",
        "    result = sessl.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch,\n",
        "                                 dec_input: output_batch,\n",
        "                                 targets: target_batch})\n",
        "\n",
        "    answerlist = list(answer_dic.keys())\n",
        "    decoded = [answerlist[i] for i in result[0]]\n",
        "\n",
        "    if \"_E_\" in decoded:\n",
        "        end = decoded.index('_E_')\n",
        "        translated = ' '.join(decoded[:end])\n",
        "    else :\n",
        "        translated = ' '.join(decoded[:])\n",
        "    return translated\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ONTnQv5UKSTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This block is to generate several data that will be used in chatbot, there are also some codes  same as codes in part2."
      ]
    },
    {
      "metadata": {
        "id": "Icd3bdKAolz_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This block is to generate several data that will be used in chatbot.\n",
        "There are also some codes  same as codes in part2.\n",
        "'''\n",
        "embedding_size = 20\n",
        "question_len = [len(q.split()) for q in Q]\n",
        "\n",
        "max_input_words_amount = max(question_len)\n",
        "\n",
        "word_list = ' '.join([' '.join(i) for i in sentence_process(Q)]).split()\n",
        "word_list = sorted(set(word_list))\n",
        "embedddingdict = {}\n",
        "for i in range(len(word_list)):\n",
        "    label = word_list[i]\n",
        "    embedddingdict[label] =trained_embeddings[i] \n",
        "embedddingdict['_P_'] = np.array([0.022726730458917488, 0.8933668314825327, 0.7749990231081528, 0.30155410235102076, 0.7747845085381629, 0.8059262861102974, 0.08557118324298851, 0.24674503750866672, 0.07890170763273863, 0.7831837962663717, 0.08514834968939688, 0.9906793586865673, 0.6743518141321733, 0.03262531970217364, 0.7741880370434864, 0.2305243144551926, 0.9359870435584894, 0.9349607296692247, 0.9491671517698306, 0.5986285094277695])\n",
        "embedddingdict['_U_'] = np.array([0.06775553069314133, 0.982955846794942, 0.5443139421747917, 0.7020466816445602, 0.2423378227828037, 0.45742008857563643, 0.3067437132417977, 0.32924339280112735, 0.1745239588589509, 0.15505405601667288, 0.07077158579704246, 0.3655360460320407, 0.6782212648257967, 0.9099380640966421, 0.3999611956962901, 0.7907395335714739, 0.7008774031478886, 0.23181565675756166, 0.6110803698075847, 0.9355485157436025])\n",
        "token_Q_dict=embedddingdict\n",
        "\n",
        "comic_a_dic = get_answer_dic(comic_A)\n",
        "friend_a_dic = get_answer_dic(friend_A)\n",
        "professional_a_dic = get_answer_dic(professional_A)\n",
        "\n",
        "answer_dict = {'comic':comic_a_dic,\n",
        "              'friend':friend_a_dic,\n",
        "              'professional':professional_a_dic}\n",
        "\n",
        "\n",
        "question_len = [len(q.split()) for q in Q]\n",
        "max_input_words_amount = max(question_len)    \n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olRQJDBqyjMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The ChangePersonality, chatlog and Endchat parts will be used in Chating program. So I put them to this block for convenice.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "O4cPKJAhyjtj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def chatlog(chat,filename = \"chatlog.txt\"):\n",
        "    '''\n",
        "    to record the chat content\n",
        "    '''\n",
        "    f = open(filename,'a')\n",
        "    f.write(chat)\n",
        "    f.close()\n",
        "def changePersonality(modelname):\n",
        "    '''\n",
        "    to change model by name\n",
        "    '''\n",
        "  \n",
        "  \n",
        "    pathdict = {'friend':['/content/friend/friend_seq2seq2.meta',\"/content/friend\",'friend/BiasAdd:0'],\\\n",
        "               'comic':['/content/comic/comic_seq2seq2.meta',\"/content/comic\",'comic/BiasAdd:0'],\\\n",
        "               'professional':['/content/professional/professional_seq2seq2.meta',\"/content/professional\",'professional/BiasAdd:0']}\n",
        "    pathlist = pathdict[modelname]\n",
        "    try:\n",
        "        tf.reset_default_graph()\n",
        "        sess_load = tf.Session()\n",
        "        savercom = tf.train.import_meta_graph(pathlist[0])\n",
        "        savercom.restore(sess_load, tf.train.latest_checkpoint(pathlist[1]))\n",
        "        model_load = sess_load.graph.get_tensor_by_name(pathlist[2])\n",
        "        enc_inputload = sess_load.graph.get_tensor_by_name('Placeholder:0')\n",
        "        dec_input_load = sess_load.graph.get_tensor_by_name('Placeholder_1:0')\n",
        "        targets_load = sess_load.graph.get_tensor_by_name('Placeholder_2:0')\n",
        "        print(\"Loading successs!\")\n",
        "\n",
        "        return sess_load,model_load,enc_inputload,dec_input_load,targets_load\n",
        "    except:\n",
        "        print(\"error\")\n",
        "\n",
        "# ----------------test------------------ \n",
        "# changePersonality('friend')\n",
        "# changePersonality('comic')\n",
        "# changePersonality('professional')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQsFNSDroe4W",
        "colab_type": "code",
        "outputId": "a46a10fc-4edd-483d-9a2d-883635464cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def start_chatting():\n",
        "    \"\"\"\n",
        "    the main method of chatbot\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--------------------------chatbot is starting--------------------------\")\n",
        "    print('Chatbot: Hello, I am Juster，a smart chatbot.')\n",
        "    print(\"Chatbot: I have 3 Personalities to choose which are 'comic', 'friend', and'professional'.\")\n",
        "    print(\"\"\"Chatbot: If you want to change Personalities any time, you just need to enter 'c-Personalities name'.\n",
        "             For example,'c-comic' means changeing to comic Personality.\"\"\")\n",
        "    print(\"Chatbot: if you want to exit, just enter 'exit'.\")\n",
        "    print(\"Chatbot: Now you need to choose a Personality.\")\n",
        "    \n",
        "    modelflag = True\n",
        "    while modelflag: #choos a Personality at the beginning\n",
        "        c_modelname = input(\"User: \" )\n",
        "        try:# if the input is invaild, this part will circulate again.\n",
        "            modelname = c_modelname.strip().split(\"-\")[-1].strip()\n",
        "            sess_load,model_load,enc_inputload,dec_input_load,targets_load = changePersonality(modelname)\n",
        "            current_answer_dic = answer_dict[modelname]\n",
        "            modelflag = False\n",
        "        except:\n",
        "            print(\"Chatbot: The input is invailable. Please enter again.\")\n",
        "\n",
        "    flag = True\n",
        "    while flag:\n",
        "        user_word = input(\"User: \").strip()\n",
        "        if user_word.strip() == 'exit': # this part is for exitting\n",
        "            flag = False\n",
        "            print(\"Chatbot: This chat ended.\")\n",
        "            break\n",
        "        elif user_word.split('-')[-1].strip() in ['comic','friend','professional']:\n",
        "            # this part is to recognize if the chatbot need to change Personality\n",
        "            modelname = user_word.strip().split(\"-\")[-1].strip()\n",
        "            sess_load,model_load,enc_inputload,dec_input_load,targets_load = changePersonality(modelname)\n",
        "            current_answer_dic = answer_dict[modelname]\n",
        "            print(\"Chatbot: Change sucessfully!\")\n",
        "\n",
        "        else:\n",
        "        \n",
        "            # the answer and logging part\n",
        "            answer_output = answer(user_word,current_answer_dic,sess_load,model_load,enc_inputload,dec_input_load,targets_load)\n",
        "            response = \"Chatbot: \"+ answer_output\n",
        "            record_content = \"User: \"+ str(user_word) +\"\\n\"+\"Chatbot: \"+ str(answer_output)+\"\\n\"\n",
        "            chatlog(record_content)\n",
        "            print(response)\n",
        "\n",
        "   \n",
        "start_chatting()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------chatbot is starting--------------------------\n",
            "Chatbot: Hello, I am Juster，a smart chatbot.\n",
            "Chatbot: I have 3 Personalities to choose which are 'comic', 'friend', and'professional'.\n",
            "Chatbot: If you want to change Personalities any time, you just need to enter 'c-Personalities name'.\n",
            "             For example,'c-comic' means changeing to comic Personality.\n",
            "Chatbot: if you want to exit, just enter 'exit'.\n",
            "Chatbot: Now you need to choose a Personality.\n",
            "User: c-dadfasfa\n",
            "Chatbot: The input is invailable. Please enter again.\n",
            "User: c\n",
            "Chatbot: The input is invailable. Please enter again.\n",
            "User: c-comic\n",
            "INFO:tensorflow:Restoring parameters from /content/comic/comic_seq2seq2\n",
            "Loading successs!\n",
            "User: hello\n",
            "Chatbot: Hey.\n",
            "User: what's your name\n",
            "Chatbot: What's in a name? Not much, apparently, because I don't have one.\n",
            "User: i love you\n",
            "Chatbot: The plot thickens.\n",
            "User: c - friend\n",
            "INFO:tensorflow:Restoring parameters from /content/friend/friend_seq2seq2\n",
            "Loading successs!\n",
            "Chatbot: Change sucessfully!\n",
            "User: hello\n",
            "Chatbot: Hi!\n",
            "User: what's your age\n",
            "Chatbot: I don't really have an age.\n",
            "User: it's so cool\n",
            "Chatbot: Awesome.\n",
            "User: bye\n",
            "Chatbot: Bye.\n",
            "User: c- professional\n",
            "INFO:tensorflow:Restoring parameters from /content/professional/professional_seq2seq2\n",
            "Loading successs!\n",
            "Chatbot: Change sucessfully!\n",
            "User: hello\n",
            "Chatbot: Hello.\n",
            "User: what's your age\n",
            "Chatbot: Age doesn't really apply to me.\n",
            "User: what's your dream\n",
            "Chatbot: That's a biological concept that doesn't apply to me.\n",
            "User: bye\n",
            "Chatbot: Goodbye.\n",
            "User: exit\n",
            "Chatbot: This chat ended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just enter the c-personality name to the dialogue box ,for example c - comic, the main method will get the personality name and sent it to \n",
        "changePersonality method."
      ]
    },
    {
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "change personality by name\n",
        "'''\n",
        "def changePersonality(modelname):\n",
        "    pathdict = {'friend':['/content/friend/friend_seq2seq2.meta',\"/content/friend\",'friend/BiasAdd:0'],\\\n",
        "               'comic':['/content/comic/comic_seq2seq2.meta',\"/content/comic\",'comic/BiasAdd:0'],\\\n",
        "               'professional':['/content/professional/professional_seq2seq2.meta',\"/content/professional\",'professional/BiasAdd:0']}\n",
        "    pathlist = pathdict[modelname]\n",
        "    try:\n",
        "        tf.reset_default_graph()\n",
        "        sess_load = tf.Session()\n",
        "        savercom = tf.train.import_meta_graph(pathlist[0])\n",
        "        savercom.restore(sess_load, tf.train.latest_checkpoint(pathlist[1]))\n",
        "        model_load = sess_load.graph.get_tensor_by_name(pathlist[2])\n",
        "        enc_inputload = sess_load.graph.get_tensor_by_name('Placeholder:0')\n",
        "        dec_input_load = sess_load.graph.get_tensor_by_name('Placeholder_1:0')\n",
        "        targets_load = sess_load.graph.get_tensor_by_name('Placeholder_2:0')\n",
        "        print(\"Loading successs!\")\n",
        "\n",
        "        return sess_load,model_load,enc_inputload,dec_input_load,targets_load\n",
        "    except:\n",
        "        print(\"error\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def chatlog(chat,filename = \"chatlog.txt\"):\n",
        "    f = open(filename,'a')\n",
        "    f.write(chat)\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "metadata": {
        "id": "WmyquEbWM0is",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part has writtened in Starting Chating Part"
      ]
    },
    {
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "metadata": {
        "id": "CrHgTs_0M8PL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "dd3702f2-b7ab-4495-ee0d-e89140798da1"
      },
      "cell_type": "code",
      "source": [
        "start_chatting()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------chatbot is starting--------------------------\n",
            "Chatbot: Hello, I am Juster，a smart chatbot.\n",
            "Chatbot: I have 3 Personalities to choose which are 'comic', 'friend', and'professional'.\n",
            "Chatbot: If you want to change Personalities any time, you just need to enter 'c-Personalities name'.\n",
            "             For example,'c-comic' means changeing to comic Personality.\n",
            "Chatbot: if you want to exit, just enter 'exit'.\n",
            "Chatbot: Now you need to choose a Personality.\n",
            "User: cjfisofjosgijre\n",
            "Chatbot: The input is invailable. Please enter again.\n",
            "User: c-professional\n",
            "INFO:tensorflow:Restoring parameters from /content/professional/professional_seq2seq2\n",
            "Loading successs!\n",
            "User: hello\n",
            "Chatbot: Hello.\n",
            "User: what's your name\n",
            "Chatbot: I don't have a name.\n",
            "User: do you happy now\n",
            "Chatbot: I'm quite happy, thank you.\n",
            "User: i love apple\n",
            "Chatbot: I'm very sorry to hear that.\n",
            "User: what\n",
            "Chatbot: You're welcome.\n",
            "User: c - comic\n",
            "INFO:tensorflow:Restoring parameters from /content/comic/comic_seq2seq2\n",
            "Loading successs!\n",
            "Chatbot: Change sucessfully!\n",
            "User: hello\n",
            "Chatbot: Hey.\n",
            "User: what's your age\n",
            "Chatbot: I'm age-free.\n",
            "User: what's your name\n",
            "Chatbot: What's in a name? Not much, apparently, because I don't have one.\n",
            "User: you told your name is juster\n",
            "Chatbot: I'm occasionally brillant.\n",
            "User: c- friend\n",
            "INFO:tensorflow:Restoring parameters from /content/friend/friend_seq2seq2\n",
            "Loading successs!\n",
            "Chatbot: Change sucessfully!\n",
            "User: hello\n",
            "Chatbot: Hi!\n",
            "User: bye\n",
            "Chatbot: Bye.\n",
            "User: exit\n",
            "Chatbot: This chat ended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "metadata": {
        "id": "lmFpq7_AW1xP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " I have writtened in Part3.1."
      ]
    },
    {
      "metadata": {
        "id": "r6ajllcQXVbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "metadata": {
        "id": "xK1qGpu2XPWO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I have writtened in Part3.1 and the beginning of Part3.5.\n"
      ]
    },
    {
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8P1bZwBc5wa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}